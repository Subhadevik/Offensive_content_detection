{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4037b09d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-03T18:37:53.625290Z",
     "iopub.status.busy": "2023-12-03T18:37:53.624902Z",
     "iopub.status.idle": "2023-12-03T18:38:06.016525Z",
     "shell.execute_reply": "2023-12-03T18:38:06.015603Z"
    },
    "papermill": {
     "duration": 12.412019,
     "end_time": "2023-12-03T18:38:06.019162",
     "exception": false,
     "start_time": "2023-12-03T18:37:53.607143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.37.2)\n",
      "Requirement already satisfied: torch in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\subha\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb58ce-17c9-47da-942c-4dcccf3028a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/span-and-text/kannada_final (1)training dataset.tsv\", sep=\"\\\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9bbe19e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:38:12.638216Z",
     "iopub.status.busy": "2023-12-03T18:38:12.637899Z",
     "iopub.status.idle": "2023-12-03T18:38:12.654873Z",
     "shell.execute_reply": "2023-12-03T18:38:12.654025Z"
    },
    "papermill": {
     "duration": 0.03621,
     "end_time": "2023-12-03T18:38:12.657003",
     "exception": false,
     "start_time": "2023-12-03T18:38:12.620793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"[0, 1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>Ade old same story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, ...</td>\n",
       "      <td>ಬ್ರೋ ಅವರು ಗೆ ದೇಶದ ಬಗ್ಗೆ ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, ...</td>\n",
       "      <td>ದೇಶಧ್ರೋಹಿಗಳು ಡಿಸ್ ಲೈಕ್ ಮಾಡಿದರೆ ನೀವು ನಿಜವಾದ ದೇಶ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>\"[55, 56, 57, 58, 59, 60, 61, 62]</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>\"[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, ...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>\"[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, ...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>\"[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>\"[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, ...</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "0                             \"[0, 1, 2, 3, 4, 5, 6, 7]   \n",
       "1     \"[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, ...   \n",
       "2     \"[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, ...   \n",
       "3     \"[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]   \n",
       "4         \"[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]   \n",
       "...                                                 ...   \n",
       "1795                  \"[55, 56, 57, 58, 59, 60, 61, 62]   \n",
       "1796  \"[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, ...   \n",
       "1797  \"[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, ...   \n",
       "1798  \"[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]   \n",
       "1799  \"[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, ...   \n",
       "\n",
       "                                                   text  \n",
       "0                                    Ade old same story  \n",
       "1             ಬ್ರೋ ಅವರು ಗೆ ದೇಶದ ಬಗ್ಗೆ ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ  \n",
       "2     ದೇಶಧ್ರೋಹಿಗಳು ಡಿಸ್ ಲೈಕ್ ಮಾಡಿದರೆ ನೀವು ನಿಜವಾದ ದೇಶ...  \n",
       "3                  Bindu gowda ge super agi ugididdeera  \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...  \n",
       "...                                                 ...  \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...  \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...  \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...  \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...  \n",
       "1799                     8k dislike sure all vijay fans  \n",
       "\n",
       "[1800 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1452b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:38:12.691557Z",
     "iopub.status.busy": "2023-12-03T18:38:12.691265Z",
     "iopub.status.idle": "2023-12-03T18:38:12.698148Z",
     "shell.execute_reply": "2023-12-03T18:38:12.697339Z"
    },
    "papermill": {
     "duration": 0.025889,
     "end_time": "2023-12-03T18:38:12.699903",
     "exception": false,
     "start_time": "2023-12-03T18:38:12.674014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rem(text):\n",
    "    return(text[1:])\n",
    "df['spans'] = df['spans'].apply(rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324e59a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:38:12.734029Z",
     "iopub.status.busy": "2023-12-03T18:38:12.733744Z",
     "iopub.status.idle": "2023-12-03T18:38:12.824560Z",
     "shell.execute_reply": "2023-12-03T18:38:12.823734Z"
    },
    "papermill": {
     "duration": 0.110001,
     "end_time": "2023-12-03T18:38:12.826498",
     "exception": false,
     "start_time": "2023-12-03T18:38:12.716497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>Ade old same story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...</td>\n",
       "      <td>ಬ್ರೋ ಅವರು ಗೆ ದೇಶದ ಬಗ್ಗೆ ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...</td>\n",
       "      <td>ದೇಶಧ್ರೋಹಿಗಳು ಡಿಸ್ ಲೈಕ್ ಮಾಡಿದರೆ ನೀವು ನಿಜವಾದ ದೇಶ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>[55, 56, 57, 58, 59, 60, 61, 62]</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "0                              [0, 1, 2, 3, 4, 5, 6, 7]   \n",
       "1     [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...   \n",
       "2     [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...   \n",
       "3      [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]   \n",
       "4          [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]   \n",
       "...                                                 ...   \n",
       "1795                   [55, 56, 57, 58, 59, 60, 61, 62]   \n",
       "1796  [83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...   \n",
       "1797  [24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...   \n",
       "1798   [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]   \n",
       "1799  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n",
       "\n",
       "                                                   text  \n",
       "0                                    Ade old same story  \n",
       "1             ಬ್ರೋ ಅವರು ಗೆ ದೇಶದ ಬಗ್ಗೆ ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ  \n",
       "2     ದೇಶಧ್ರೋಹಿಗಳು ಡಿಸ್ ಲೈಕ್ ಮಾಡಿದರೆ ನೀವು ನಿಜವಾದ ದೇಶ...  \n",
       "3                  Bindu gowda ge super agi ugididdeera  \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...  \n",
       "...                                                 ...  \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...  \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...  \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...  \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...  \n",
       "1799                     8k dislike sure all vijay fans  \n",
       "\n",
       "[1800 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df['text'].tolist()\n",
    "spans = [eval(span) for span in df['spans'].tolist()]\n",
    "spans[1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4764f080",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:38:12.861213Z",
     "iopub.status.busy": "2023-12-03T18:38:12.860929Z",
     "iopub.status.idle": "2023-12-03T18:38:43.262675Z",
     "shell.execute_reply": "2023-12-03T18:38:43.261668Z"
    },
    "papermill": {
     "duration": 30.421639,
     "end_time": "2023-12-03T18:38:43.265107",
     "exception": false,
     "start_time": "2023-12-03T18:38:12.843468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\r\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\r\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\r\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\r\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\r\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\r\n",
      "Building wheels for collected packages: googletrans\r\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=4ee9a6e8f3c3ca56abae52b9616b727771660f3742746a22478528d9c8d5ad5c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\r\n",
      "Successfully built googletrans\r\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\r\n",
      "  Attempting uninstall: h11\r\n",
      "    Found existing installation: h11 0.14.0\r\n",
      "    Uninstalling h11-0.14.0:\r\n",
      "      Successfully uninstalled h11-0.14.0\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.4\r\n",
      "    Uninstalling idna-3.4:\r\n",
      "      Successfully uninstalled idna-3.4\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "jupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.0 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\r\n",
      "tensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\r\n",
      "Collecting langdetect\r\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from langdetect) (1.16.0)\r\n",
      "Building wheels for collected packages: langdetect\r\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=30e2b5a64701fa971a2bc26673d21a279c3fd64a4a0ebd7b9e70b5f525274890\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\r\n",
      "Successfully built langdetect\r\n",
      "Installing collected packages: langdetect\r\n",
      "Successfully installed langdetect-1.0.9\r\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0-rc1\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9076efad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:38:43.307335Z",
     "iopub.status.busy": "2023-12-03T18:38:43.306947Z",
     "iopub.status.idle": "2023-12-03T18:45:48.064022Z",
     "shell.execute_reply": "2023-12-03T18:45:48.063040Z"
    },
    "papermill": {
     "duration": 424.801489,
     "end_time": "2023-12-03T18:45:48.086759",
     "exception": false,
     "start_time": "2023-12-03T18:38:43.285270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>translated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>Ade old same story</td>\n",
       "      <td>Ade old same story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...</td>\n",
       "      <td>ಬ್ರೋ ಅವರು ಗೆ ದೇಶದ ಬಗ್ಗೆ ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ</td>\n",
       "      <td>Bro let him be good about the country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...</td>\n",
       "      <td>ದೇಶಧ್ರೋಹಿಗಳು ಡಿಸ್ ಲೈಕ್ ಮಾಡಿದರೆ ನೀವು ನಿಜವಾದ ದೇಶ...</td>\n",
       "      <td>If the patriots discriminate, you should be sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>[55, 56, 57, 58, 59, 60, 61, 62]</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "0                              [0, 1, 2, 3, 4, 5, 6, 7]   \n",
       "1     [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...   \n",
       "2     [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...   \n",
       "3      [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]   \n",
       "4          [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]   \n",
       "...                                                 ...   \n",
       "1795                   [55, 56, 57, 58, 59, 60, 61, 62]   \n",
       "1796  [83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...   \n",
       "1797  [24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...   \n",
       "1798   [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]   \n",
       "1799  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n",
       "\n",
       "                                                   text  \\\n",
       "0                                    Ade old same story   \n",
       "1             ಬ್ರೋ ಅವರು ಗೆ ದೇಶದ ಬಗ್ಗೆ ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ   \n",
       "2     ದೇಶಧ್ರೋಹಿಗಳು ಡಿಸ್ ಲೈಕ್ ಮಾಡಿದರೆ ನೀವು ನಿಜವಾದ ದೇಶ...   \n",
       "3                  Bindu gowda ge super agi ugididdeera   \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...   \n",
       "...                                                 ...   \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...   \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...   \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...   \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...   \n",
       "1799                     8k dislike sure all vijay fans   \n",
       "\n",
       "                                        translated_text  \n",
       "0                                    Ade old same story  \n",
       "1                 Bro let him be good about the country  \n",
       "2     If the patriots discriminate, you should be sh...  \n",
       "3                  Bindu gowda ge super agi ugididdeera  \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...  \n",
       "...                                                 ...  \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...  \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...  \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...  \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...  \n",
       "1799                     8k dislike sure all vijay fans  \n",
       "\n",
       "[1800 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator, LANGUAGES\n",
    "from langdetect import detect\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def translate_or_transliterate(text):\n",
    "    try:\n",
    "        # Detect the language of the text\n",
    "        detected_lang = detect(text)\n",
    "\n",
    "        # If the text is in Kannada, translate it to English\n",
    "        if detected_lang == 'kn':\n",
    "            return translator.translate(text, src='kn', dest='en').text\n",
    "        \n",
    "#         # If the text is Kannada written in Roman script, transliterate (this is a placeholder, actual transliteration might need a different approach)\n",
    "#         if detected_lang == 'en' and 'some condition to identify Romanized Kannada':\n",
    "#             # Transliterate here (this is just a placeholder)\n",
    "#             return 'Transliterated Text'\n",
    "\n",
    "        # If the text is already in English, return as is\n",
    "#         if detected_lang == 'en':\n",
    "#             return text\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    except Exception as e:\n",
    "        return text\n",
    "\n",
    "df['translated_text'] = df['text'].apply(translate_or_transliterate)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de377dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:45:48.129184Z",
     "iopub.status.busy": "2023-12-03T18:45:48.128368Z",
     "iopub.status.idle": "2023-12-03T18:45:48.133160Z",
     "shell.execute_reply": "2023-12-03T18:45:48.132359Z"
    },
    "papermill": {
     "duration": 0.028129,
     "end_time": "2023-12-03T18:45:48.135016",
     "exception": false,
     "start_time": "2023-12-03T18:45:48.106887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['text'] = df['translated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1081db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:45:48.228850Z",
     "iopub.status.busy": "2023-12-03T18:45:48.228477Z",
     "iopub.status.idle": "2023-12-03T18:45:49.299937Z",
     "shell.execute_reply": "2023-12-03T18:45:49.299156Z"
    },
    "papermill": {
     "duration": 1.095206,
     "end_time": "2023-12-03T18:45:49.302239",
     "exception": false,
     "start_time": "2023-12-03T18:45:48.207033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def convert_spans_to_bio(text, char_spans_str):\n",
    "    # Convert string representation of list to actual list\n",
    "    char_spans = ast.literal_eval(char_spans_str)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    bio_tags = ['O'] * len(tokens)\n",
    "    token_spans = []\n",
    "    start = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        token_spans.append((start, start + len(token)))\n",
    "        start += len(token) + 1  # +1 for the space\n",
    "\n",
    "    for start_char, end_char in zip(char_spans[::2], char_spans[1::2]):\n",
    "        for i, (start_token, end_token) in enumerate(token_spans):\n",
    "            if start_token < end_char and end_token > start_char:\n",
    "                bio_tags[i] = 'B' if start_token == start_char else 'I'\n",
    "\n",
    "    return bio_tags\n",
    "\n",
    "# Apply the conversion to the DataFrame\n",
    "df['bio_tags'] = df.apply(lambda row: convert_spans_to_bio(row['text'], row['spans']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c183e862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:45:49.345141Z",
     "iopub.status.busy": "2023-12-03T18:45:49.344259Z",
     "iopub.status.idle": "2023-12-03T18:45:49.364417Z",
     "shell.execute_reply": "2023-12-03T18:45:49.363149Z"
    },
    "papermill": {
     "duration": 0.044095,
     "end_time": "2023-12-03T18:45:49.366815",
     "exception": false,
     "start_time": "2023-12-03T18:45:49.322720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>Ade old same story</td>\n",
       "      <td>Ade old same story</td>\n",
       "      <td>[I, I, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...</td>\n",
       "      <td>Bro let him be good about the country</td>\n",
       "      <td>Bro let him be good about the country</td>\n",
       "      <td>[O, O, O, O, O, I, I, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...</td>\n",
       "      <td>If the patriots discriminate, you should be sh...</td>\n",
       "      <td>If the patriots discriminate, you should be sh...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, I, O, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "      <td>[O, O, O, O, O, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "      <td>[O, I, I, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>[55, 56, 57, 58, 59, 60, 61, 62]</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "      <td>[O, O, O, O, O, O, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "      <td>[O, O, O, O, I, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, I, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "      <td>[O, O, O, I, I, I]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "0                              [0, 1, 2, 3, 4, 5, 6, 7]   \n",
       "1     [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...   \n",
       "2     [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...   \n",
       "3      [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]   \n",
       "4          [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]   \n",
       "...                                                 ...   \n",
       "1795                   [55, 56, 57, 58, 59, 60, 61, 62]   \n",
       "1796  [83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...   \n",
       "1797  [24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...   \n",
       "1798   [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]   \n",
       "1799  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n",
       "\n",
       "                                                   text  \\\n",
       "0                                    Ade old same story   \n",
       "1                 Bro let him be good about the country   \n",
       "2     If the patriots discriminate, you should be sh...   \n",
       "3                  Bindu gowda ge super agi ugididdeera   \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...   \n",
       "...                                                 ...   \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...   \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...   \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...   \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...   \n",
       "1799                     8k dislike sure all vijay fans   \n",
       "\n",
       "                                        translated_text  \\\n",
       "0                                    Ade old same story   \n",
       "1                 Bro let him be good about the country   \n",
       "2     If the patriots discriminate, you should be sh...   \n",
       "3                  Bindu gowda ge super agi ugididdeera   \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...   \n",
       "...                                                 ...   \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...   \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...   \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...   \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...   \n",
       "1799                     8k dislike sure all vijay fans   \n",
       "\n",
       "                                               bio_tags  \n",
       "0                                          [I, I, O, O]  \n",
       "1                              [O, O, O, O, O, I, I, I]  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, I, O, I, I, ...  \n",
       "3                                    [O, O, O, O, O, I]  \n",
       "4                                 [O, I, I, O, O, O, O]  \n",
       "...                                                 ...  \n",
       "1795                              [O, O, O, O, O, O, I]  \n",
       "1796  [O, O, O, O, O, O, O, O, O, O, O, O, O, I, I, ...  \n",
       "1797  [O, O, O, O, I, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1798               [O, O, O, O, O, O, O, O, O, O, I, I]  \n",
       "1799                                 [O, O, O, I, I, I]  \n",
       "\n",
       "[1800 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1190b961",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:45:49.419297Z",
     "iopub.status.busy": "2023-12-03T18:45:49.418469Z",
     "iopub.status.idle": "2023-12-03T18:45:58.786172Z",
     "shell.execute_reply": "2023-12-03T18:45:58.785149Z"
    },
    "papermill": {
     "duration": 9.392057,
     "end_time": "2023-12-03T18:45:58.788771",
     "exception": false,
     "start_time": "2023-12-03T18:45:49.396714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 5000  # Adjust as needed\n",
    "max_len = 128  # Max length of sequences\n",
    "embedding_dim = 100  # Embedding dimensions\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "X = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert BIO tags to numerical format and pad\n",
    "tag2idx = {'O': 0, 'B': 1, 'I': 2}\n",
    "y = [[tag2idx.get(tag) for tag in bio_tags] for bio_tags in df['bio_tags']]\n",
    "y = pad_sequences(y, maxlen=max_len, padding='post', value=tag2idx['O'])\n",
    "y = np.array([np.eye(len(tag2idx))[i] for i in y])  # One-hot encoding\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91753b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:45:58.832818Z",
     "iopub.status.busy": "2023-12-03T18:45:58.831695Z",
     "iopub.status.idle": "2023-12-03T18:46:10.942269Z",
     "shell.execute_reply": "2023-12-03T18:46:10.941088Z"
    },
    "papermill": {
     "duration": 12.134625,
     "end_time": "2023-12-03T18:46:10.944701",
     "exception": false,
     "start_time": "2023-12-03T18:45:58.810076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf2crf\r\n",
      "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\r\n",
      "Requirement already satisfied: tensorflow>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from tf2crf) (2.13.0)\r\n",
      "Requirement already satisfied: tensorflow-addons>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from tf2crf) (0.22.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (23.5.26)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (0.2.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (1.51.1)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (3.9.0)\r\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (2.13.1)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (16.0.6)\r\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (1.24.3)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (68.1.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (2.13.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (2.13.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (4.5.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (1.15.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.1.0->tf2crf) (0.34.0)\r\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons>=0.8.2->tf2crf) (2.13.3)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.1.0->tf2crf) (0.41.2)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (2.22.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (1.0.0)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (3.4.4)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (2.31.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (0.7.1)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (3.0.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow>=2.1.0->tf2crf) (3.0.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (0.2.7)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (4.9)\r\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (1.26.15)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (1.3.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (2023.7.22)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (2.1.3)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow>=2.1.0->tf2crf) (3.2.2)\r\n",
      "Installing collected packages: tf2crf\r\n",
      "Successfully installed tf2crf-0.1.33\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tf2crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8821b7e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:10.990729Z",
     "iopub.status.busy": "2023-12-03T18:46:10.989853Z",
     "iopub.status.idle": "2023-12-03T18:46:11.009127Z",
     "shell.execute_reply": "2023-12-03T18:46:11.008095Z"
    },
    "papermill": {
     "duration": 0.043473,
     "end_time": "2023-12-03T18:46:11.011026",
     "exception": false,
     "start_time": "2023-12-03T18:46:10.967553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "      <th>translated_text</th>\n",
       "      <th>bio_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7]</td>\n",
       "      <td>Ade old same story</td>\n",
       "      <td>Ade old same story</td>\n",
       "      <td>[I, I, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...</td>\n",
       "      <td>Bro let him be good about the country</td>\n",
       "      <td>Bro let him be good about the country</td>\n",
       "      <td>[O, O, O, O, O, I, I, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...</td>\n",
       "      <td>If the patriots discriminate, you should be sh...</td>\n",
       "      <td>If the patriots discriminate, you should be sh...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, I, O, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "      <td>Bindu gowda ge super agi ugididdeera</td>\n",
       "      <td>[O, O, O, O, O, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "      <td>Kannadadalli mado badlu hindinalli madidre sup...</td>\n",
       "      <td>[O, I, I, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>[55, 56, 57, 58, 59, 60, 61, 62]</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "      <td>Ivarukkku eppodhum thalaivar kalaigner lightaa...</td>\n",
       "      <td>[O, O, O, O, O, O, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>[83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "      <td>Trailer Nala irukanu oru than comment pandranu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "      <td>Wigpathy Visay na Padam Flop than ithula Kabal...</td>\n",
       "      <td>[O, O, O, O, I, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>[63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "      <td>Vikram ella styleum set aaguthu.. Namba moonji...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, I, I]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "      <td>8k dislike sure all vijay fans</td>\n",
       "      <td>[O, O, O, I, I, I]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  spans  \\\n",
       "0                              [0, 1, 2, 3, 4, 5, 6, 7]   \n",
       "1     [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 3...   \n",
       "2     [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 6...   \n",
       "3      [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]   \n",
       "4          [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]   \n",
       "...                                                 ...   \n",
       "1795                   [55, 56, 57, 58, 59, 60, 61, 62]   \n",
       "1796  [83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 9...   \n",
       "1797  [24, 25, 26, 27, 28, 29, 102, 103, 104, 105, 1...   \n",
       "1798   [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]   \n",
       "1799  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2...   \n",
       "\n",
       "                                                   text  \\\n",
       "0                                    Ade old same story   \n",
       "1                 Bro let him be good about the country   \n",
       "2     If the patriots discriminate, you should be sh...   \n",
       "3                  Bindu gowda ge super agi ugididdeera   \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...   \n",
       "...                                                 ...   \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...   \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...   \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...   \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...   \n",
       "1799                     8k dislike sure all vijay fans   \n",
       "\n",
       "                                        translated_text  \\\n",
       "0                                    Ade old same story   \n",
       "1                 Bro let him be good about the country   \n",
       "2     If the patriots discriminate, you should be sh...   \n",
       "3                  Bindu gowda ge super agi ugididdeera   \n",
       "4     Kannadadalli mado badlu hindinalli madidre sup...   \n",
       "...                                                 ...   \n",
       "1795  Ivarukkku eppodhum thalaivar kalaigner lightaa...   \n",
       "1796  Trailer Nala irukanu oru than comment pandranu...   \n",
       "1797  Wigpathy Visay na Padam Flop than ithula Kabal...   \n",
       "1798  Vikram ella styleum set aaguthu.. Namba moonji...   \n",
       "1799                     8k dislike sure all vijay fans   \n",
       "\n",
       "                                               bio_tags  \n",
       "0                                          [I, I, O, O]  \n",
       "1                              [O, O, O, O, O, I, I, I]  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, I, O, I, I, ...  \n",
       "3                                    [O, O, O, O, O, I]  \n",
       "4                                 [O, I, I, O, O, O, O]  \n",
       "...                                                 ...  \n",
       "1795                              [O, O, O, O, O, O, I]  \n",
       "1796  [O, O, O, O, O, O, O, O, O, O, O, O, O, I, I, ...  \n",
       "1797  [O, O, O, O, I, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1798               [O, O, O, O, O, O, O, O, O, O, I, I]  \n",
       "1799                                 [O, O, O, I, I, I]  \n",
       "\n",
       "[1800 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b7c4b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:11.056223Z",
     "iopub.status.busy": "2023-12-03T18:46:11.055272Z",
     "iopub.status.idle": "2023-12-03T18:46:16.814991Z",
     "shell.execute_reply": "2023-12-03T18:46:16.814034Z"
    },
    "papermill": {
     "duration": 5.787337,
     "end_time": "2023-12-03T18:46:16.819762",
     "exception": false,
     "start_time": "2023-12-03T18:46:11.032425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 128, 100)          500000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 100)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 128, 256)          234496    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 128, 3)            771       \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 735267 (2.80 MB)\n",
      "Trainable params: 735267 (2.80 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dropout, Bidirectional, LSTM, TimeDistributed, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Model Architecture without CRF\n",
    "input = Input(shape=(max_len,))\n",
    "model = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len)(input)\n",
    "model = Dropout(0.1)(model)\n",
    "model = Bidirectional(LSTM(units=128, return_sequences=True))(model)\n",
    "out = TimeDistributed(Dense(len(tag2idx), activation=\"softmax\"))(model)  # Output layer\n",
    "\n",
    "model = Model(input, out)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "466f7356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:16.869180Z",
     "iopub.status.busy": "2023-12-03T18:46:16.868833Z",
     "iopub.status.idle": "2023-12-03T18:46:53.563846Z",
     "shell.execute_reply": "2023-12-03T18:46:53.562913Z"
    },
    "papermill": {
     "duration": 36.722748,
     "end_time": "2023-12-03T18:46:53.566127",
     "exception": false,
     "start_time": "2023-12-03T18:46:16.843379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 17s 205ms/step - loss: 0.2144 - accuracy: 0.9418 - val_loss: 0.0807 - val_accuracy: 0.9636\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 7s 143ms/step - loss: 0.0763 - accuracy: 0.9636 - val_loss: 0.0786 - val_accuracy: 0.9644\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 5s 104ms/step - loss: 0.0705 - accuracy: 0.9673 - val_loss: 0.0794 - val_accuracy: 0.9622\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 5s 92ms/step - loss: 0.0651 - accuracy: 0.9720 - val_loss: 0.0782 - val_accuracy: 0.9641\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 2s 42ms/step - loss: 0.0594 - accuracy: 0.9759 - val_loss: 0.0779 - val_accuracy: 0.9648\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2824a138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:53.647798Z",
     "iopub.status.busy": "2023-12-03T18:46:53.647398Z",
     "iopub.status.idle": "2023-12-03T18:46:53.672424Z",
     "shell.execute_reply": "2023-12-03T18:46:53.671627Z"
    },
    "papermill": {
     "duration": 0.068187,
     "end_time": "2023-12-03T18:46:53.674681",
     "exception": false,
     "start_time": "2023-12-03T18:46:53.606494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"/kaggle/input/span-and-text/kannada_test_EACL24 (1).csv\",header=None,names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1fb564c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:53.753437Z",
     "iopub.status.busy": "2023-12-03T18:46:53.753057Z",
     "iopub.status.idle": "2023-12-03T18:46:53.765014Z",
     "shell.execute_reply": "2023-12-03T18:46:53.764001Z"
    },
    "papermill": {
     "duration": 0.053306,
     "end_time": "2023-12-03T18:46:53.767219",
     "exception": false,
     "start_time": "2023-12-03T18:46:53.713913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#rashmikamandhana  bad girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#ಇದು ಚರಿತ್ರೆ ಸ್ರೃಷ್ಟಿಸುವ ಅವತಾರ nBaground music 🤺🤘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@#win 12 nodidini sir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@A.R.W   tumbad tanhaji andhadhun aise bahot h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Ajaya shetty sari bidi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Yes bro avaga song ajneesh sir voice alli ittu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Yes hange kick kodta hogutte.. ಒಳ್ಳೆ synch ಮಾಡ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Yes song super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>You have bright future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Yudha mada beku katukara🧟 munde oda ‍baradu bh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0                          #rashmikamandhana  bad girl\n",
       "1    #ಇದು ಚರಿತ್ರೆ ಸ್ರೃಷ್ಟಿಸುವ ಅವತಾರ nBaground music 🤺🤘\n",
       "2                                @#win 12 nodidini sir\n",
       "3    @A.R.W   tumbad tanhaji andhadhun aise bahot h...\n",
       "4                              @Ajaya shetty sari bidi\n",
       "..                                                 ...\n",
       "439     Yes bro avaga song ajneesh sir voice alli ittu\n",
       "440  Yes hange kick kodta hogutte.. ಒಳ್ಳೆ synch ಮಾಡ...\n",
       "441                                     Yes song super\n",
       "442                             You have bright future\n",
       "443  Yudha mada beku katukara🧟 munde oda ‍baradu bh...\n",
       "\n",
       "[444 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd1754a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:53.838764Z",
     "iopub.status.busy": "2023-12-03T18:46:53.838431Z",
     "iopub.status.idle": "2023-12-03T18:46:54.761257Z",
     "shell.execute_reply": "2023-12-03T18:46:54.760244Z"
    },
    "papermill": {
     "duration": 0.961548,
     "end_time": "2023-12-03T18:46:54.763922",
     "exception": false,
     "start_time": "2023-12-03T18:46:53.802374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.86551136e-01, 2.07148753e-02, 4.92734045e-01],\n",
       "       [4.86104369e-01, 2.09144540e-02, 4.92981106e-01],\n",
       "       [6.97252929e-01, 1.80555321e-02, 2.84691572e-01],\n",
       "       [9.01204705e-01, 1.00619039e-02, 8.87334347e-02],\n",
       "       [9.62265491e-01, 4.94811684e-03, 3.27863060e-02],\n",
       "       [9.88090158e-01, 2.02982710e-03, 9.87993088e-03],\n",
       "       [9.96515155e-01, 7.59714399e-04, 2.72514066e-03],\n",
       "       [9.98918533e-01, 2.89466203e-04, 7.91976403e-04],\n",
       "       [9.99604523e-01, 1.23272082e-04, 2.72280391e-04],\n",
       "       [9.99822676e-01, 6.13195152e-05, 1.16123214e-04],\n",
       "       [9.99904037e-01, 3.54748154e-05, 6.04764537e-05],\n",
       "       [9.99939919e-01, 2.32211660e-05, 3.68531801e-05],\n",
       "       [9.99958158e-01, 1.66953487e-05, 2.52005721e-05],\n",
       "       [9.99968410e-01, 1.28729689e-05, 1.87328642e-05],\n",
       "       [9.99974728e-01, 1.04599685e-05, 1.48059844e-05],\n",
       "       [9.99978900e-01, 8.84466226e-06, 1.22534748e-05],\n",
       "       [9.99981761e-01, 7.71201576e-06, 1.05047675e-05],\n",
       "       [9.99983788e-01, 6.88778982e-06, 9.25626500e-06],\n",
       "       [9.99985337e-01, 6.26964356e-06, 8.33493959e-06],\n",
       "       [9.99986529e-01, 5.79444395e-06, 7.63658227e-06],\n",
       "       [9.99987483e-01, 5.42157022e-06, 7.09540609e-06],\n",
       "       [9.99988198e-01, 5.12391716e-06, 6.66823053e-06],\n",
       "       [9.99988794e-01, 4.88283877e-06, 6.32578440e-06],\n",
       "       [9.99989271e-01, 4.68514963e-06, 6.04761544e-06],\n",
       "       [9.99989629e-01, 4.52132144e-06, 5.81909399e-06],\n",
       "       [9.99989986e-01, 4.38430652e-06, 5.62953937e-06],\n",
       "       [9.99990225e-01, 4.26880342e-06, 5.47094805e-06],\n",
       "       [9.99990463e-01, 4.17074898e-06, 5.33729872e-06],\n",
       "       [9.99990702e-01, 4.08700771e-06, 5.22392384e-06],\n",
       "       [9.99990821e-01, 4.01510488e-06, 5.12720544e-06],\n",
       "       [9.99991059e-01, 3.95306824e-06, 5.04427135e-06],\n",
       "       [9.99991059e-01, 3.89932347e-06, 4.97283418e-06],\n",
       "       [9.99991298e-01, 3.85258772e-06, 4.91105811e-06],\n",
       "       [9.99991298e-01, 3.81180917e-06, 4.85743158e-06],\n",
       "       [9.99991417e-01, 3.77612355e-06, 4.81073630e-06],\n",
       "       [9.99991536e-01, 3.74480896e-06, 4.76995456e-06],\n",
       "       [9.99991536e-01, 3.71726514e-06, 4.73423870e-06],\n",
       "       [9.99991655e-01, 3.69298346e-06, 4.70289251e-06],\n",
       "       [9.99991655e-01, 3.67153416e-06, 4.67530572e-06],\n",
       "       [9.99991655e-01, 3.65256346e-06, 4.65099356e-06],\n",
       "       [9.99991775e-01, 3.63574327e-06, 4.62952221e-06],\n",
       "       [9.99991775e-01, 3.62082301e-06, 4.61053287e-06],\n",
       "       [9.99991775e-01, 3.60755303e-06, 4.59370540e-06],\n",
       "       [9.99991894e-01, 3.59575165e-06, 4.57877377e-06],\n",
       "       [9.99991894e-01, 3.58523948e-06, 4.56551425e-06],\n",
       "       [9.99991894e-01, 3.57586282e-06, 4.55371674e-06],\n",
       "       [9.99991894e-01, 3.56750047e-06, 4.54321980e-06],\n",
       "       [9.99991894e-01, 3.56002670e-06, 4.53386201e-06],\n",
       "       [9.99991894e-01, 3.55334441e-06, 4.52551149e-06],\n",
       "       [9.99991894e-01, 3.54737199e-06, 4.51805545e-06],\n",
       "       [9.99991894e-01, 3.54202393e-06, 4.51139522e-06],\n",
       "       [9.99991894e-01, 3.53723726e-06, 4.50544030e-06],\n",
       "       [9.99991894e-01, 3.53294217e-06, 4.50010657e-06],\n",
       "       [9.99991894e-01, 3.52909001e-06, 4.49533718e-06],\n",
       "       [9.99991894e-01, 3.52564507e-06, 4.49106028e-06],\n",
       "       [9.99991894e-01, 3.52254642e-06, 4.48722449e-06],\n",
       "       [9.99991894e-01, 3.51976951e-06, 4.48379387e-06],\n",
       "       [9.99991894e-01, 3.51727294e-06, 4.48070750e-06],\n",
       "       [9.99992013e-01, 3.51503013e-06, 4.47793946e-06],\n",
       "       [9.99992013e-01, 3.51301924e-06, 4.47545517e-06],\n",
       "       [9.99992013e-01, 3.51121753e-06, 4.47321872e-06],\n",
       "       [9.99992013e-01, 3.50959021e-06, 4.47121010e-06],\n",
       "       [9.99992132e-01, 3.50812502e-06, 4.46939930e-06],\n",
       "       [9.99992132e-01, 3.50681375e-06, 4.46777540e-06],\n",
       "       [9.99992132e-01, 3.50563664e-06, 4.46631839e-06],\n",
       "       [9.99992132e-01, 3.50458708e-06, 4.46500235e-06],\n",
       "       [9.99992132e-01, 3.50363484e-06, 4.46381455e-06],\n",
       "       [9.99992132e-01, 3.50278606e-06, 4.46275044e-06],\n",
       "       [9.99992132e-01, 3.50202777e-06, 4.46178410e-06],\n",
       "       [9.99992132e-01, 3.50134678e-06, 4.46091644e-06],\n",
       "       [9.99992132e-01, 3.50074242e-06, 4.46013382e-06],\n",
       "       [9.99992132e-01, 3.50020491e-06, 4.45942760e-06],\n",
       "       [9.99992132e-01, 3.49972447e-06, 4.45878140e-06],\n",
       "       [9.99992132e-01, 3.49930360e-06, 4.45820297e-06],\n",
       "       [9.99992132e-01, 3.49894003e-06, 4.45768001e-06],\n",
       "       [9.99992132e-01, 3.49862307e-06, 4.45719934e-06],\n",
       "       [9.99992132e-01, 3.49835636e-06, 4.45676596e-06],\n",
       "       [9.99992132e-01, 3.49813604e-06, 4.45637488e-06],\n",
       "       [9.99992132e-01, 3.49796233e-06, 4.45601381e-06],\n",
       "       [9.99992132e-01, 3.49783545e-06, 4.45568685e-06],\n",
       "       [9.99992132e-01, 3.49775542e-06, 4.45538080e-06],\n",
       "       [9.99992132e-01, 3.49772222e-06, 4.45510432e-06],\n",
       "       [9.99992132e-01, 3.49774882e-06, 4.45484966e-06],\n",
       "       [9.99992132e-01, 3.49782886e-06, 4.45461592e-06],\n",
       "       [9.99992132e-01, 3.49797574e-06, 4.45439946e-06],\n",
       "       [9.99992132e-01, 3.49819265e-06, 4.45418709e-06],\n",
       "       [9.99992132e-01, 3.49848960e-06, 4.45399564e-06],\n",
       "       [9.99992132e-01, 3.49888319e-06, 4.45381738e-06],\n",
       "       [9.99992132e-01, 3.49939728e-06, 4.45365595e-06],\n",
       "       [9.99992132e-01, 3.50002801e-06, 4.45349042e-06],\n",
       "       [9.99992132e-01, 3.50082564e-06, 4.45334172e-06],\n",
       "       [9.99992132e-01, 3.50180767e-06, 4.45320575e-06],\n",
       "       [9.99992132e-01, 3.50302344e-06, 4.45307842e-06],\n",
       "       [9.99992132e-01, 3.50451046e-06, 4.45297655e-06],\n",
       "       [9.99992132e-01, 3.50633900e-06, 4.45289152e-06],\n",
       "       [9.99992132e-01, 3.50858681e-06, 4.45284468e-06],\n",
       "       [9.99992132e-01, 3.51134463e-06, 4.45285332e-06],\n",
       "       [9.99992132e-01, 3.51475205e-06, 4.45295109e-06],\n",
       "       [9.99992013e-01, 3.51896097e-06, 4.45317983e-06],\n",
       "       [9.99992013e-01, 3.52417328e-06, 4.45359183e-06],\n",
       "       [9.99992013e-01, 3.53066253e-06, 4.45429259e-06],\n",
       "       [9.99992013e-01, 3.53874270e-06, 4.45538444e-06],\n",
       "       [9.99992013e-01, 3.54884764e-06, 4.45702926e-06],\n",
       "       [9.99992013e-01, 3.56151122e-06, 4.45942715e-06],\n",
       "       [9.99992013e-01, 3.57740146e-06, 4.46281365e-06],\n",
       "       [9.99992013e-01, 3.59737783e-06, 4.46746390e-06],\n",
       "       [9.99991894e-01, 3.62251581e-06, 4.47364073e-06],\n",
       "       [9.99991775e-01, 3.65415281e-06, 4.48157834e-06],\n",
       "       [9.99991775e-01, 3.69399845e-06, 4.49137679e-06],\n",
       "       [9.99991775e-01, 3.74419915e-06, 4.50285370e-06],\n",
       "       [9.99991655e-01, 3.80754500e-06, 4.51538381e-06],\n",
       "       [9.99991536e-01, 3.88774788e-06, 4.52762970e-06],\n",
       "       [9.99991536e-01, 3.98996872e-06, 4.53737266e-06],\n",
       "       [9.99991298e-01, 4.12173722e-06, 4.54133260e-06],\n",
       "       [9.99991179e-01, 4.29472493e-06, 4.53552366e-06],\n",
       "       [9.99990940e-01, 4.52794666e-06, 4.51624192e-06],\n",
       "       [9.99990582e-01, 4.85353348e-06, 4.48228275e-06],\n",
       "       [9.99990225e-01, 5.32701961e-06, 4.43856970e-06],\n",
       "       [9.99989510e-01, 6.04506886e-06, 4.40045642e-06],\n",
       "       [9.99988437e-01, 7.17519106e-06, 4.39717996e-06],\n",
       "       [9.99986410e-01, 9.00284613e-06, 4.47215325e-06],\n",
       "       [9.99983311e-01, 1.19982133e-05, 4.67993277e-06],\n",
       "       [9.99977946e-01, 1.68924344e-05, 5.08278799e-06],\n",
       "       [9.99969602e-01, 2.47324115e-05, 5.75219337e-06],\n",
       "       [9.99956369e-01, 3.68741421e-05, 6.77938988e-06],\n",
       "       [9.99936819e-01, 5.48897078e-05, 8.29829696e-06],\n",
       "       [9.99909163e-01, 8.03740768e-05, 1.05267700e-05],\n",
       "       [9.99871612e-01, 1.14598457e-04, 1.38432833e-05]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert spans to BIO tags for the new dataset\n",
    "# new_df['bio_tags'] = new_df.apply(lambda row: convert_spans_to_bio(row['text'], row['spans']), axis=1)\n",
    "\n",
    "# Tokenize and pad new dataset\n",
    "new_sequences = tokenizer.texts_to_sequences(new_df['text'])\n",
    "X_new = pad_sequences(new_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "predictions[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e7fbaf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:54.837763Z",
     "iopub.status.busy": "2023-12-03T18:46:54.837412Z",
     "iopub.status.idle": "2023-12-03T18:46:54.844200Z",
     "shell.execute_reply": "2023-12-03T18:46:54.843382Z"
    },
    "papermill": {
     "duration": 0.045491,
     "end_time": "2023-12-03T18:46:54.846110",
     "exception": false,
     "start_time": "2023-12-03T18:46:54.800619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value_indices = np.argmax(predictions[1], axis=1)\n",
    "max_value_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a761a491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:54.921648Z",
     "iopub.status.busy": "2023-12-03T18:46:54.921315Z",
     "iopub.status.idle": "2023-12-03T18:46:54.934620Z",
     "shell.execute_reply": "2023-12-03T18:46:54.933651Z"
    },
    "papermill": {
     "duration": 0.051829,
     "end_time": "2023-12-03T18:46:54.936541",
     "exception": false,
     "start_time": "2023-12-03T18:46:54.884712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(predictions)):    \n",
    "    x = np.argmax(predictions[i], axis=1)\n",
    "    y = np.where(x == 2)[0]\n",
    "    res.append(list(y))\n",
    "res\n",
    "new_df['final_span']=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17193256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.009570Z",
     "iopub.status.busy": "2023-12-03T18:46:55.009246Z",
     "iopub.status.idle": "2023-12-03T18:46:55.023431Z",
     "shell.execute_reply": "2023-12-03T18:46:55.022608Z"
    },
    "papermill": {
     "duration": 0.052734,
     "end_time": "2023-12-03T18:46:55.025327",
     "exception": false,
     "start_time": "2023-12-03T18:46:54.972593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>final_span</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#rashmikamandhana  bad girl</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#ಇದು ಚರಿತ್ರೆ ಸ್ರೃಷ್ಟಿಸುವ ಅವತಾರ nBaground music 🤺🤘</td>\n",
       "      <td>[3, 4, 5, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@#win 12 nodidini sir</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@A.R.W   tumbad tanhaji andhadhun aise bahot h...</td>\n",
       "      <td>[33, 34]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Ajaya shetty sari bidi</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Yes bro avaga song ajneesh sir voice alli ittu</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Yes hange kick kodta hogutte.. ಒಳ್ಳೆ synch ಮಾಡ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Yes song super</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>You have bright future</td>\n",
       "      <td>[0, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Yudha mada beku katukara🧟 munde oda ‍baradu bh...</td>\n",
       "      <td>[5, 6, 7, 8]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text    final_span\n",
       "0                          #rashmikamandhana  bad girl        [0, 1]\n",
       "1    #ಇದು ಚರಿತ್ರೆ ಸ್ರೃಷ್ಟಿಸುವ ಅವತಾರ nBaground music 🤺🤘  [3, 4, 5, 6]\n",
       "2                                @#win 12 nodidini sir            []\n",
       "3    @A.R.W   tumbad tanhaji andhadhun aise bahot h...      [33, 34]\n",
       "4                              @Ajaya shetty sari bidi            []\n",
       "..                                                 ...           ...\n",
       "439     Yes bro avaga song ajneesh sir voice alli ittu            []\n",
       "440  Yes hange kick kodta hogutte.. ಒಳ್ಳೆ synch ಮಾಡ...            []\n",
       "441                                     Yes song super            []\n",
       "442                             You have bright future        [0, 2]\n",
       "443  Yudha mada beku katukara🧟 munde oda ‍baradu bh...  [5, 6, 7, 8]\n",
       "\n",
       "[444 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5db74594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.099175Z",
     "iopub.status.busy": "2023-12-03T18:46:55.098873Z",
     "iopub.status.idle": "2023-12-03T18:46:55.108351Z",
     "shell.execute_reply": "2023-12-03T18:46:55.107488Z"
    },
    "papermill": {
     "duration": 0.048236,
     "end_time": "2023-12-03T18:46:55.110328",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.062092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1ddb0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.184109Z",
     "iopub.status.busy": "2023-12-03T18:46:55.183791Z",
     "iopub.status.idle": "2023-12-03T18:46:55.201290Z",
     "shell.execute_reply": "2023-12-03T18:46:55.200474Z"
    },
    "papermill": {
     "duration": 0.056695,
     "end_time": "2023-12-03T18:46:55.203106",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.146411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_span\n",
       "[]                                                                                 214\n",
       "[1]                                                                                 25\n",
       "[1, 2]                                                                              13\n",
       "[0, 1]                                                                              10\n",
       "[2]                                                                                  9\n",
       "                                                                                  ... \n",
       "[2, 4]                                                                               1\n",
       "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]      1\n",
       "[10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]                         1\n",
       "[4, 5, 6, 7, 8, 10]                                                                  1\n",
       "[0, 2]                                                                               1\n",
       "Name: count, Length: 108, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['final_span'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cadc030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.637140Z",
     "iopub.status.busy": "2023-12-03T18:46:55.636761Z",
     "iopub.status.idle": "2023-12-03T18:46:55.641287Z",
     "shell.execute_reply": "2023-12-03T18:46:55.640364Z"
    },
    "papermill": {
     "duration": 0.043529,
     "end_time": "2023-12-03T18:46:55.643160",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.599631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Assuming predictions is the output from model.predict\n",
    "\n",
    "# # Function to convert numerical indices back to BIO tags\n",
    "# def indices_to_tags(predictions, idx2tag):\n",
    "#     tag_sequences = []\n",
    "#     for sequence in predictions:\n",
    "#         tag_sequence = [idx2tag[np.argmax(tag)] for tag in sequence]\n",
    "#         tag_sequences.append(tag_sequence)\n",
    "#     return tag_sequences\n",
    "\n",
    "# # Create an inverse mapping from numerical index to tag\n",
    "# idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "# # Convert predictions to BIO tags\n",
    "# bio_tag_predictions = indices_to_tags(predictions, idx2tag)\n",
    "# bio_tag_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51d4e544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.717412Z",
     "iopub.status.busy": "2023-12-03T18:46:55.717057Z",
     "iopub.status.idle": "2023-12-03T18:46:55.721296Z",
     "shell.execute_reply": "2023-12-03T18:46:55.720389Z"
    },
    "papermill": {
     "duration": 0.04382,
     "end_time": "2023-12-03T18:46:55.723251",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.679431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # new_df = pd.read_csv(\"/kaggle/input/span-and-text/kannada_test_EACL24 (1).csv\",header=None, names=['text'] )\n",
    "# new_df = df\n",
    "# new_sequences = tokenizer.texts_to_sequences(new_df['translated_text'])\n",
    "# new_X = pad_sequences(new_sequences, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a54dd23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.797783Z",
     "iopub.status.busy": "2023-12-03T18:46:55.797432Z",
     "iopub.status.idle": "2023-12-03T18:46:55.801453Z",
     "shell.execute_reply": "2023-12-03T18:46:55.800508Z"
    },
    "papermill": {
     "duration": 0.043662,
     "end_time": "2023-12-03T18:46:55.803542",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.759880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff92584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.877577Z",
     "iopub.status.busy": "2023-12-03T18:46:55.877241Z",
     "iopub.status.idle": "2023-12-03T18:46:55.881221Z",
     "shell.execute_reply": "2023-12-03T18:46:55.880375Z"
    },
    "papermill": {
     "duration": 0.043432,
     "end_time": "2023-12-03T18:46:55.883262",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.839830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Predict using the model\n",
    "# predictions = model.predict(new_X)\n",
    "# predicted_bio_tags = np.argmax(predictions, axis=-1)  # Convert probabilities to class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42c8303d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:55.957562Z",
     "iopub.status.busy": "2023-12-03T18:46:55.956808Z",
     "iopub.status.idle": "2023-12-03T18:46:55.961451Z",
     "shell.execute_reply": "2023-12-03T18:46:55.960640Z"
    },
    "papermill": {
     "duration": 0.043846,
     "end_time": "2023-12-03T18:46:55.963523",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.919677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def bio_tags_to_spans(bio_sequence, tokenizer):\n",
    "#     spans = []\n",
    "#     current_span = []\n",
    "\n",
    "#     for idx, tag in enumerate(bio_sequence):\n",
    "#         if tag == tag2idx['B']:\n",
    "#             if current_span:\n",
    "#                 spans.append(current_span)\n",
    "#             current_span = [idx]\n",
    "#         elif tag == tag2idx['I']:\n",
    "#             if current_span:\n",
    "#                 current_span.append(idx)\n",
    "#         else:\n",
    "#             if current_span:\n",
    "#                 spans.append(current_span)\n",
    "#                 current_span = []\n",
    "\n",
    "#     # Add the last span\n",
    "#     if current_span:\n",
    "#         spans.append(current_span)\n",
    "\n",
    "#     return spans\n",
    "\n",
    "# # Apply the conversion to predicted BIO tags\n",
    "# new_df['predicted_spans'] = [bio_tags_to_spans(bio_sequence, tokenizer) for bio_sequence in predicted_bio_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01c7177b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.037102Z",
     "iopub.status.busy": "2023-12-03T18:46:56.036470Z",
     "iopub.status.idle": "2023-12-03T18:46:56.040495Z",
     "shell.execute_reply": "2023-12-03T18:46:56.039610Z"
    },
    "papermill": {
     "duration": 0.042819,
     "end_time": "2023-12-03T18:46:56.042310",
     "exception": false,
     "start_time": "2023-12-03T18:46:55.999491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df['predicted_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "110e746b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.115997Z",
     "iopub.status.busy": "2023-12-03T18:46:56.115456Z",
     "iopub.status.idle": "2023-12-03T18:46:56.120179Z",
     "shell.execute_reply": "2023-12-03T18:46:56.119374Z"
    },
    "papermill": {
     "duration": 0.043649,
     "end_time": "2023-12-03T18:46:56.122062",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.078413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Inspecting more data from the training set\n",
    "# sample_size = 100  # Increase sample size\n",
    "# sample_indices = np.random.choice(range(len(X_train)), size=sample_size, replace=False)\n",
    "# sample_X = X_train[sample_indices]\n",
    "# sample_y_true = y_train[sample_indices]\n",
    "\n",
    "# sample_predictions = model.predict(sample_X)\n",
    "# sample_predicted_bio_tags = np.argmax(sample_predictions, axis=-1)\n",
    "\n",
    "# empty_true_spans, empty_predicted_spans = 0, 0\n",
    "\n",
    "# for i in range(sample_size):\n",
    "#     true_spans = bio_tags_to_spans(sample_y_true[i], tag2idx)\n",
    "#     predicted_spans = bio_tags_to_spans(sample_predicted_bio_tags[i], tag2idx)\n",
    "\n",
    "#     if not true_spans:\n",
    "#         empty_true_spans += 1\n",
    "#     if not predicted_spans:\n",
    "#         empty_predicted_spans += 1\n",
    "\n",
    "#     if i < 10:  # Print first 10 for inspection\n",
    "#         print(f\"True spans: {true_spans}\")\n",
    "#         print(f\"Predicted spans: {predicted_spans}\")\n",
    "#         print()\n",
    "\n",
    "# print(f\"Empty true spans in sample: {empty_true_spans}/{sample_size}\")\n",
    "# print(f\"Empty predicted spans in sample: {empty_predicted_spans}/{sample_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e29649",
   "metadata": {
    "papermill": {
     "duration": 0.036066,
     "end_time": "2023-12-03T18:46:56.194412",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.158346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "507955c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.269186Z",
     "iopub.status.busy": "2023-12-03T18:46:56.268862Z",
     "iopub.status.idle": "2023-12-03T18:46:56.274170Z",
     "shell.execute_reply": "2023-12-03T18:46:56.273328Z"
    },
    "papermill": {
     "duration": 0.045533,
     "end_time": "2023-12-03T18:46:56.276125",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.230592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import ast\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "# def convert_char_spans_to_token_labels(text, char_spans, tokenizer):\n",
    "#     token_labels = ['O'] * len(text)  # Initialize all labels as 'O'\n",
    "#     char_spans = ast.literal_eval(char_spans)  # Convert string representation to list\n",
    "\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     token_positions = tokenizer(text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "\n",
    "#     for start, end in zip(char_spans[::2], char_spans[1::2]):\n",
    "#         for idx, (tok_start, tok_end) in enumerate(token_positions):\n",
    "#             if tok_start >= start and tok_end <= end:\n",
    "#                 token_labels[idx] = 'B' if tok_start == start else 'I'\n",
    "    \n",
    "#     return tokens, token_labels\n",
    "\n",
    "# # Example usage with the tokenizer from a pre-trained model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# # Assuming df is your DataFrame with 'spans' and 'text' columns\n",
    "# df['token_labels'] = df.apply(lambda x: convert_char_spans_to_token_labels(x['text'], x['spans'], tokenizer), axis=1)\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# # Define label-to-index mapping\n",
    "# label2idx = {'O': 0, 'B': 1, 'I': 2}\n",
    "\n",
    "# def align_labels_with_tokens(token_labels_list, label2idx):\n",
    "#     aligned_labels_list = []\n",
    "#     for token_labels in token_labels_list:\n",
    "#         aligned_labels = [label2idx[label] if label in label2idx else label2idx['O'] for label in token_labels]\n",
    "#         aligned_labels_list.append(aligned_labels)\n",
    "#     return aligned_labels_list\n",
    "\n",
    "# df['aligned_labels'] = df['token_labels'].apply(lambda x: align_labels_with_tokens(x, label2idx))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54465fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.351000Z",
     "iopub.status.busy": "2023-12-03T18:46:56.350289Z",
     "iopub.status.idle": "2023-12-03T18:46:56.356720Z",
     "shell.execute_reply": "2023-12-03T18:46:56.355871Z"
    },
    "papermill": {
     "duration": 0.046199,
     "end_time": "2023-12-03T18:46:56.358656",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.312457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AdamW\n",
    "\n",
    "# # Assuming df is your DataFrame with 'text' and 'aligned_labels'\n",
    "\n",
    "# class NERDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_len):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = self.texts[idx]\n",
    "#         labels = self.labels[idx]\n",
    "\n",
    "#         encoding = self.tokenizer.encode_plus(\n",
    "#             text,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_len,\n",
    "#             return_token_type_ids=False,\n",
    "#             padding='max_length',\n",
    "#             truncation=True,\n",
    "#             return_attention_mask=True,\n",
    "#             return_tensors='pt',\n",
    "#         )\n",
    "\n",
    "#         # Adjust labels\n",
    "#         label_length = len(labels)\n",
    "#         adjusted_labels = labels + [0] * (self.max_len - label_length)\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': encoding['input_ids'].flatten(),\n",
    "#             'attention_mask': encoding['attention_mask'].flatten(),\n",
    "#             'labels': torch.tensor(adjusted_labels, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# # Define some parameters\n",
    "# max_len = 128\n",
    "# batch_size = 16\n",
    "\n",
    "# # Prepare the dataset\n",
    "# dataset = NERDataset(df['text'].tolist(), df['aligned_labels'].tolist(), tokenizer, max_len)\n",
    "# data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "# model_name = \"bert-base-multilingual-cased\"  # or \"bert-base-cased\" for English\n",
    "\n",
    "# # Load the model\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label2idx))\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "\n",
    "# # Optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# for epoch in range(3):  # num of epochs\n",
    "#     for batch in data_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408cc59",
   "metadata": {
    "papermill": {
     "duration": 0.03622,
     "end_time": "2023-12-03T18:46:56.431187",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.394967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d9ce6",
   "metadata": {
    "papermill": {
     "duration": 0.035859,
     "end_time": "2023-12-03T18:46:56.503327",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.467468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086aeb86",
   "metadata": {
    "papermill": {
     "duration": 0.035943,
     "end_time": "2023-12-03T18:46:56.575280",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.539337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa1b829a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.649430Z",
     "iopub.status.busy": "2023-12-03T18:46:56.648694Z",
     "iopub.status.idle": "2023-12-03T18:46:56.652804Z",
     "shell.execute_reply": "2023-12-03T18:46:56.651909Z"
    },
    "papermill": {
     "duration": 0.043311,
     "end_time": "2023-12-03T18:46:56.654571",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.611260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# texts = df['text'].tolist()\n",
    "# spans = [eval(span) for span in df['spans'].tolist()]\n",
    "# spans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cd8c6b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.728471Z",
     "iopub.status.busy": "2023-12-03T18:46:56.727679Z",
     "iopub.status.idle": "2023-12-03T18:46:56.731863Z",
     "shell.execute_reply": "2023-12-03T18:46:56.730894Z"
    },
    "papermill": {
     "duration": 0.043052,
     "end_time": "2023-12-03T18:46:56.733743",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.690691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# i=0\n",
    "# for txt in texts:\n",
    "#     print(spans[i])\n",
    "#     for sp in spans[i]:\n",
    "#         print(txt[sp])\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbd4006a",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.808408Z",
     "iopub.status.busy": "2023-12-03T18:46:56.807754Z",
     "iopub.status.idle": "2023-12-03T18:46:56.811819Z",
     "shell.execute_reply": "2023-12-03T18:46:56.810946Z"
    },
    "papermill": {
     "duration": 0.043152,
     "end_time": "2023-12-03T18:46:56.813719",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.770567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temp_lst = []\n",
    "# i=0\n",
    "# for txt in texts:    \n",
    "#     temp=\"\"\n",
    "# #     print(spans[i])\n",
    "#     for sp in spans[i]:\n",
    "#         try:\n",
    "#             temp+=txt[int(sp)]\n",
    "#         except:\n",
    "#             continue\n",
    "#     i=i+1\n",
    "# #     print(temp)\n",
    "#     temp_lst.append(temp)\n",
    "# df['span_words']=temp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a39de1ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.886860Z",
     "iopub.status.busy": "2023-12-03T18:46:56.886555Z",
     "iopub.status.idle": "2023-12-03T18:46:56.890246Z",
     "shell.execute_reply": "2023-12-03T18:46:56.889425Z"
    },
    "papermill": {
     "duration": 0.042363,
     "end_time": "2023-12-03T18:46:56.892168",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.849805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e749e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:56.965974Z",
     "iopub.status.busy": "2023-12-03T18:46:56.965633Z",
     "iopub.status.idle": "2023-12-03T18:46:56.969462Z",
     "shell.execute_reply": "2023-12-03T18:46:56.968620Z"
    },
    "papermill": {
     "duration": 0.043399,
     "end_time": "2023-12-03T18:46:56.971507",
     "exception": false,
     "start_time": "2023-12-03T18:46:56.928108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df = df[['text','span_words']]\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e15db794",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.045480Z",
     "iopub.status.busy": "2023-12-03T18:46:57.045148Z",
     "iopub.status.idle": "2023-12-03T18:46:57.049501Z",
     "shell.execute_reply": "2023-12-03T18:46:57.048650Z"
    },
    "papermill": {
     "duration": 0.04369,
     "end_time": "2023-12-03T18:46:57.051447",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.007757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def bio_tagging(sentence, phrase):\n",
    "#     tokens = sentence.split()\n",
    "#     phrase_tokens = phrase.split()\n",
    "\n",
    "#     tags = ['O'] * len(tokens)\n",
    "\n",
    "#     try:\n",
    "#         start_idx = tokens.index(phrase_tokens[0])\n",
    "\n",
    "#         tags[start_idx] = 'B'\n",
    "#         for i in range(1, len(phrase_tokens)):\n",
    "#             if start_idx + i < len(tokens):\n",
    "#                 tags[start_idx + i] = 'I'\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "\n",
    "#     return tags\n",
    "\n",
    "# # Create BIO tags for each sentence\n",
    "# df['bio_tags'] = df.apply(lambda row: bio_tagging(row['text'], row['span_words']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c6b36cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.125087Z",
     "iopub.status.busy": "2023-12-03T18:46:57.124774Z",
     "iopub.status.idle": "2023-12-03T18:46:57.128745Z",
     "shell.execute_reply": "2023-12-03T18:46:57.127899Z"
    },
    "papermill": {
     "duration": 0.043064,
     "end_time": "2023-12-03T18:46:57.130708",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.087644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4db8e9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.204789Z",
     "iopub.status.busy": "2023-12-03T18:46:57.204464Z",
     "iopub.status.idle": "2023-12-03T18:46:57.210252Z",
     "shell.execute_reply": "2023-12-03T18:46:57.209377Z"
    },
    "papermill": {
     "duration": 0.045219,
     "end_time": "2023-12-03T18:46:57.212120",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.166901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizerFast\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# def bio_tagging(sentence, phrase):\n",
    "#     tokens = sentence.split()\n",
    "#     phrase_tokens = phrase.split()\n",
    "\n",
    "#     tags = ['O'] * len(tokens)\n",
    "\n",
    "#     try:\n",
    "#         start_idx = tokens.index(phrase_tokens[0])\n",
    "\n",
    "#         tags[start_idx] = 'B'\n",
    "#         for i in range(1, len(phrase_tokens)):\n",
    "#             if start_idx + i < len(tokens):\n",
    "#                 tags[start_idx + i] = 'I'\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "\n",
    "#     return tags\n",
    "\n",
    "# df['bio_tags'] = df.apply(lambda row: bio_tagging(row['text'], row['span_words']), axis=1)\n",
    "\n",
    "# class SentenceDataset(Dataset):\n",
    "#     def __init__(self, sentences, tags, tokenizer, max_len):\n",
    "#         self.sentences = sentences\n",
    "#         self.tags = tags\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         sentence = self.sentences[idx]\n",
    "#         word_labels = self.tags[idx]\n",
    "\n",
    "#         encoding = self.tokenizer(sentence.split(), is_split_into_words=True, return_offsets_mapping=True, padding='max_length', truncation=True, max_length=self.max_len)\n",
    "#         labels = [label for word in word_labels for label in [word] + ['O'] * (len(self.tokenizer.tokenize(word)) - 1)]\n",
    "\n",
    "#         # Padding\n",
    "#         labels += ['O'] * (self.max_len - len(labels))\n",
    "\n",
    "#         return {\n",
    "#             'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "#             'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "#             'labels': torch.tensor([label_to_id[label] for label in labels], dtype=torch.long)  # Convert labels to numerical IDs\n",
    "#         }\n",
    "\n",
    "# label_to_id = {'O': 0, 'B': 1, 'I': 2}\n",
    "\n",
    "# dataset = SentenceDataset(df['text'].tolist(), df['bio_tags'].tolist(), tokenizer, max_len=128)\n",
    "\n",
    "# # Split dataset into training and validation\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# # DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# # Model training setup (not shown here for brevity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "790fe5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.288135Z",
     "iopub.status.busy": "2023-12-03T18:46:57.287361Z",
     "iopub.status.idle": "2023-12-03T18:46:57.293154Z",
     "shell.execute_reply": "2023-12-03T18:46:57.292258Z"
    },
    "papermill": {
     "duration": 0.046732,
     "end_time": "2023-12-03T18:46:57.295176",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.248444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from transformers import BertForTokenClassification, AdamW\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import torch\n",
    "\n",
    "# # ... [Previous code for data preparation and DataLoader setup] ...\n",
    "\n",
    "# # Model setup\n",
    "# model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3) # Adjust num_labels based on your tagging scheme\n",
    "\n",
    "# # Training setup\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 3  # Adjust the number of epochs based on your requirement\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     avg_train_loss = total_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "#     # Validation step\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_loader:\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "\n",
    "#             outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "\n",
    "#             total_val_loss += loss.item()\n",
    "\n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c212707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.368955Z",
     "iopub.status.busy": "2023-12-03T18:46:57.368625Z",
     "iopub.status.idle": "2023-12-03T18:46:57.373749Z",
     "shell.execute_reply": "2023-12-03T18:46:57.372887Z"
    },
    "papermill": {
     "duration": 0.043975,
     "end_time": "2023-12-03T18:46:57.375503",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.331528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def extract_substring(model, tokenizer, text, device):\n",
    "#     # Tokenize the input text\n",
    "#     inputs = tokenizer.encode_plus(text, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "#     # Move tensors to the same device as the model\n",
    "#     input_ids = inputs['input_ids'].to(device)\n",
    "#     attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "#     # Predict\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#     logits = outputs.logits\n",
    "#     # Convert logits to label IDs\n",
    "#     label_ids = torch.argmax(logits, dim=2)\n",
    "# #     print(label_ids)\n",
    "\n",
    "#     # Convert IDs to labels\n",
    "#     labels = [id_to_label[label_id] for label_id in label_ids[0].cpu().numpy()]\n",
    "\n",
    "#     # Extract tokens corresponding to 'B' and 'I' tags\n",
    "#     tokens = tokenizer.convert_ids_to_tokens(input_ids[0].cpu().numpy())\n",
    "#     extracted_tokens = [token for token, label in zip(tokens, labels) if label in [\"B\", \"I\"]]\n",
    "\n",
    "#     # Convert tokens back to string\n",
    "#     extracted_string = tokenizer.convert_tokens_to_string(extracted_tokens)\n",
    "# #     print(\"ext_str\",extracted_string)\n",
    "#     if(extracted_string==\"\"):\n",
    "#         return \"nil\"\n",
    "#     return extracted_string.strip()\n",
    "\n",
    "# # Usage\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# id_to_label = {0: 'O', 1: 'B', 2: 'I'}  # Reverse mapping from ID to label\n",
    "\n",
    "# # Example text\n",
    "# text = \"@Troll Stupid Fans heno helkodthidallla  adunna helkodo munche neenu kaltho henge irbeku antha suvar gandu anthidde ivga ninge baididke thika huritha\"\n",
    "# extracted_substring = extract_substring(model, tokenizer, text, device)\n",
    "# print(extracted_substring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70edbf53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.449003Z",
     "iopub.status.busy": "2023-12-03T18:46:57.448691Z",
     "iopub.status.idle": "2023-12-03T18:46:57.452775Z",
     "shell.execute_reply": "2023-12-03T18:46:57.451832Z"
    },
    "papermill": {
     "duration": 0.043225,
     "end_time": "2023-12-03T18:46:57.454724",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.411499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df['pred_span'] = new_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e0c3df3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.528281Z",
     "iopub.status.busy": "2023-12-03T18:46:57.527958Z",
     "iopub.status.idle": "2023-12-03T18:46:57.531905Z",
     "shell.execute_reply": "2023-12-03T18:46:57.530998Z"
    },
    "papermill": {
     "duration": 0.043064,
     "end_time": "2023-12-03T18:46:57.533750",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.490686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df['extracted_substring'] = new_df['text'].apply(lambda x: extract_substring(model, tokenizer, x, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "334a057d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.607452Z",
     "iopub.status.busy": "2023-12-03T18:46:57.607136Z",
     "iopub.status.idle": "2023-12-03T18:46:57.611036Z",
     "shell.execute_reply": "2023-12-03T18:46:57.610220Z"
    },
    "papermill": {
     "duration": 0.043033,
     "end_time": "2023-12-03T18:46:57.613028",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.569995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6aa945db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.687259Z",
     "iopub.status.busy": "2023-12-03T18:46:57.686476Z",
     "iopub.status.idle": "2023-12-03T18:46:57.690432Z",
     "shell.execute_reply": "2023-12-03T18:46:57.689590Z"
    },
    "papermill": {
     "duration": 0.043322,
     "end_time": "2023-12-03T18:46:57.692508",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.649186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df['extracted_substring'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22d0c38b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.767745Z",
     "iopub.status.busy": "2023-12-03T18:46:57.766915Z",
     "iopub.status.idle": "2023-12-03T18:46:57.771080Z",
     "shell.execute_reply": "2023-12-03T18:46:57.770185Z"
    },
    "papermill": {
     "duration": 0.044601,
     "end_time": "2023-12-03T18:46:57.773116",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.728515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2=pd.read_csv(\"/kaggle/input/span-and-text/kannada_test_EACL24 (1).csv\",header=None, names=['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6157575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.848703Z",
     "iopub.status.busy": "2023-12-03T18:46:57.847871Z",
     "iopub.status.idle": "2023-12-03T18:46:57.851885Z",
     "shell.execute_reply": "2023-12-03T18:46:57.851019Z"
    },
    "papermill": {
     "duration": 0.043958,
     "end_time": "2023-12-03T18:46:57.853792",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.809834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e98110c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:57.929188Z",
     "iopub.status.busy": "2023-12-03T18:46:57.928108Z",
     "iopub.status.idle": "2023-12-03T18:46:57.932554Z",
     "shell.execute_reply": "2023-12-03T18:46:57.931814Z"
    },
    "papermill": {
     "duration": 0.044102,
     "end_time": "2023-12-03T18:46:57.934465",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.890363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2['extracted_substring'] = df2['text'].apply(lambda x: extract_substring(model, tokenizer, x, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8b6cb11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:58.008586Z",
     "iopub.status.busy": "2023-12-03T18:46:58.008275Z",
     "iopub.status.idle": "2023-12-03T18:46:58.012468Z",
     "shell.execute_reply": "2023-12-03T18:46:58.011666Z"
    },
    "papermill": {
     "duration": 0.043366,
     "end_time": "2023-12-03T18:46:58.014444",
     "exception": false,
     "start_time": "2023-12-03T18:46:57.971078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d318164e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:58.089773Z",
     "iopub.status.busy": "2023-12-03T18:46:58.089432Z",
     "iopub.status.idle": "2023-12-03T18:46:58.093318Z",
     "shell.execute_reply": "2023-12-03T18:46:58.092562Z"
    },
    "papermill": {
     "duration": 0.042702,
     "end_time": "2023-12-03T18:46:58.095223",
     "exception": false,
     "start_time": "2023-12-03T18:46:58.052521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2['extracted_substring'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2108f04d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:58.169077Z",
     "iopub.status.busy": "2023-12-03T18:46:58.168762Z",
     "iopub.status.idle": "2023-12-03T18:46:58.172792Z",
     "shell.execute_reply": "2023-12-03T18:46:58.171943Z"
    },
    "papermill": {
     "duration": 0.043354,
     "end_time": "2023-12-03T18:46:58.174748",
     "exception": false,
     "start_time": "2023-12-03T18:46:58.131394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5760bd5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:58.248789Z",
     "iopub.status.busy": "2023-12-03T18:46:58.248447Z",
     "iopub.status.idle": "2023-12-03T18:46:58.252448Z",
     "shell.execute_reply": "2023-12-03T18:46:58.251619Z"
    },
    "papermill": {
     "duration": 0.043859,
     "end_time": "2023-12-03T18:46:58.254881",
     "exception": false,
     "start_time": "2023-12-03T18:46:58.211022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/kaggle/input/span-and-text/kannada_final (1).tsv\", sep=\"\\\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "386fba10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:58.329434Z",
     "iopub.status.busy": "2023-12-03T18:46:58.328703Z",
     "iopub.status.idle": "2023-12-03T18:46:58.332754Z",
     "shell.execute_reply": "2023-12-03T18:46:58.331871Z"
    },
    "papermill": {
     "duration": 0.043128,
     "end_time": "2023-12-03T18:46:58.334720",
     "exception": false,
     "start_time": "2023-12-03T18:46:58.291592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def rem(text):\n",
    "#     return(text[1:])\n",
    "# df['spans'] = df['spans'].apply(rem)\n",
    "# # df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "00058f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-03T18:46:58.408473Z",
     "iopub.status.busy": "2023-12-03T18:46:58.408186Z",
     "iopub.status.idle": "2023-12-03T18:46:58.414668Z",
     "shell.execute_reply": "2023-12-03T18:46:58.413795Z"
    },
    "papermill": {
     "duration": 0.045618,
     "end_time": "2023-12-03T18:46:58.416489",
     "exception": false,
     "start_time": "2023-12-03T18:46:58.370871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sklearn_crfsuite\n",
    "# from sklearn_crfsuite import metrics\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Example DataFrame\n",
    "# data = {\n",
    "#     'text': [\"Ade old same story\", \"ಬ್ರೋ ಅವರು ಗೆ ದೇಶದ ಬಗ್ಗೆ ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ\", \"...\"],\n",
    "#     'span_words': [\"Ade old\", \"ಅಭಿಮಾನ ಇಲ್ಲಾ ಬಿಡಿ\", \"...\"]\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# def bio_tagging(sentence, phrase):\n",
    "#     tokens = sentence.split()\n",
    "#     phrase_tokens = phrase.split()\n",
    "\n",
    "#     tags = ['O'] * len(tokens)\n",
    "\n",
    "#     try:\n",
    "#         start_idx = tokens.index(phrase_tokens[0])\n",
    "\n",
    "#         tags[start_idx] = 'B'\n",
    "#         for i in range(1, len(phrase_tokens)):\n",
    "#             if start_idx + i < len(tokens):\n",
    "#                 tags[start_idx + i] = 'I'\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "\n",
    "#     return tags\n",
    "\n",
    "# # Adding BIO tags to the DataFrame\n",
    "# df['bio_tags'] = df.apply(lambda row: bio_tagging(row['text'], row['span_words']), axis=1)\n",
    "\n",
    "# # Feature extraction function\n",
    "# def word2features(sent, i):\n",
    "#     word = sent[i]\n",
    "\n",
    "#     features = {\n",
    "#         'bias': 1.0,\n",
    "#         'word.lower()': word.lower(),\n",
    "#         'word[-3:]': word[-3:],\n",
    "#         'word[-2:]': word[-2:],\n",
    "#         'word.isupper()': word.isupper(),\n",
    "#         'word.istitle()': word.istitle(),\n",
    "#         'word.isdigit()': word.isdigit(),\n",
    "#     }\n",
    "\n",
    "#     if i > 0:\n",
    "#         word1 = sent[i-1]\n",
    "#         features.update({\n",
    "#             '-1:word.lower()': word1.lower(),\n",
    "#             '-1:word.istitle()': word1.istitle(),\n",
    "#             '-1:word.isupper()': word1.isupper(),\n",
    "#         })\n",
    "#     else:\n",
    "#         features['BOS'] = True\n",
    "\n",
    "#     if i < len(sent)-1:\n",
    "#         word1 = sent[i+1]\n",
    "#         features.update({\n",
    "#             '+1:word.lower()': word1.lower(),\n",
    "#             '+1:word.istitle()': word1.istitle(),\n",
    "#             '+1:word.isupper()': word1.isupper(),\n",
    "#         })\n",
    "#     else:\n",
    "#         features['EOS'] = True\n",
    "\n",
    "#     return features\n",
    "\n",
    "# def prepare_data(df):\n",
    "#     X, y = [], []\n",
    "#     for _, row in df.iterrows():\n",
    "#         tokens = row['text'].split()\n",
    "#         bio_tags = row['bio_tags']\n",
    "\n",
    "#         X.append([word2features(tokens, i) for i in range(len(tokens))])\n",
    "#         y.append(bio_tags)\n",
    "    \n",
    "#     return X, y\n",
    "\n",
    "# X, y = prepare_data(df)\n",
    "\n",
    "# # Split data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4100573,
     "sourceId": 7111600,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 551.695336,
   "end_time": "2023-12-03T18:47:02.053930",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-03T18:37:50.358594",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
